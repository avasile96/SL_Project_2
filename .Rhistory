eliminated <- x_train
eliminated_Y <- y_train
for (v in names(x_train)){
print(v)
# Eliminating outliers
Q <- quantile(x_train[[v]], probs=c(.25, .75), na.rm = FALSE)
iqr <- IQR(x_train[[v]])
up <-  Q[2]+1.5*iqr # Upper Range
low<- Q[1]-1.5*iqr # Lower Range
eliminated_Y <- subset(eliminated_Y, eliminated[[v]] > (Q[1] - 1.5*iqr) & eliminated[[v]] < (Q[2]+1.5*iqr))
eliminated <- subset(eliminated, eliminated[[v]] > (Q[1] - 1.5*iqr) & eliminated[[v]] < (Q[2]+1.5*iqr))
}
# X Standardization
scaled.x_train <- scale(eliminated)
scaled.y_train <- scale(eliminated_Y)
### REGRESSION ###
# Start with a basic regression using the **lm(y ~ x, data)** function
lm.fit <- lm(scaled.y_train ~ .-v5-v7, data = eliminated)
plot(lm.fit)
### LOADING PACKAGES ###
library(tidyverse) #really dunno
library(eqs2lavaan) # for plotting covariance
library(GGally) # plotting correlation
### LOADING DATA ###
training_data <- as_tibble(read.csv("D:\\Uni\\SL\\SL_Project\\train_ch.csv"))
test_data <- as_tibble(read.csv("D:\\Uni\\SL\\SL_Project\\train_ch.csv"))
x_train <- training_data %>% select(2:10)
y_train <- training_data %>% select(11)
x_test <- test_data %>% select(2:10)
y_test <- test_data %>% select(11)
### EXPLORING DATA ###
# covarience
train_cov = cov(x_train)
plotCov(train_cov)
# correlation
train_cor = cor(x_train)
ggcorr(x_train)
"OBS: There's high correelation between
v1 and v5
v1 and v7
v5 and v7
So we can discard v5 and v7"
### FEATURE ENGINEERING ###
eliminated <- x_train
eliminated_Y <- y_train
for (v in names(x_train)){
print(v)
# Eliminating outliers
Q <- quantile(x_train[[v]], probs=c(.25, .75), na.rm = FALSE)
iqr <- IQR(x_train[[v]])
up <-  Q[2]+1.5*iqr # Upper Range
low<- Q[1]-1.5*iqr # Lower Range
eliminated_Y <- subset(eliminated_Y, eliminated[[v]] > (Q[1] - 1.5*iqr) & eliminated[[v]] < (Q[2]+1.5*iqr))
eliminated <- subset(eliminated, eliminated[[v]] > (Q[1] - 1.5*iqr) & eliminated[[v]] < (Q[2]+1.5*iqr))
}
# X Standardization
scaled.x_train <- scale(eliminated)
scaled.y_train <- scale(eliminated_Y)
# # check that we get mean of 0 and sd of 1
# colMeans(scaled.x_train)  # faster version of apply(scaled.dat, 2, mean)
# apply(scaled.x_train, 2, sd)
x_train$v1 = scaled.x_train[,1]
x_train$v2 = scaled.x_train[,2]
x_train$v3 = scaled.x_train[,3]
x_train$v4 = scaled.x_train[,4]
x_train$v5 = scaled.x_train[,5]
x_train$v6 = scaled.x_train[,6]
x_train$v7 = scaled.x_train[,7]
x_train$v8 = scaled.x_train[,8]
x_train$v9 = scaled.x_train[,9]
### REGRESSION ###
# Start with a basic regression using the **lm(y ~ x, data)** function
lm.fit <- lm(scaled.y_train ~ .-v5-v7, data = eliminated)
plot(lm.fit)
x_train <- training_data %>% select(2:10)
y_train <- training_data %>% select(11)
eliminated <- x_train
eliminated_Y <- y_train
for (v in names(x_train)){
print(v)
# Eliminating outliers
Q <- quantile(x_train[[v]], probs=c(.25, .75), na.rm = FALSE)
iqr <- IQR(x_train[[v]])
up <-  Q[2]+1.5*iqr # Upper Range
low<- Q[1]-1.5*iqr # Lower Range
eliminated_Y <- subset(eliminated_Y, eliminated[[v]] > (Q[1] - 1.5*iqr) & eliminated[[v]] < (Q[2]+1.5*iqr))
eliminated <- subset(eliminated, eliminated[[v]] > (Q[1] - 1.5*iqr) & eliminated[[v]] < (Q[2]+1.5*iqr))
}
# X Standardization
scaled.x_train <- scale(eliminated)
scaled.y_train <- scale(eliminated_Y)
x_train$v1 = scaled.x_train[,1]
x_train$v2 = scaled.x_train[,2]
x_train$v3 = scaled.x_train[,3]
x_train$v4 = scaled.x_train[,4]
x_train$v5 = scaled.x_train[,5]
x_train$v6 = scaled.x_train[,6]
x_train$v7 = scaled.x_train[,7]
x_train$v8 = scaled.x_train[,8]
x_train$v9 = scaled.x_train[,9]
View(x_train)
eliminated$v1 = scaled.x_train[,1]
eliminated$v2 = scaled.x_train[,2]
eliminated$v3 = scaled.x_train[,3]
eliminated$v4 = scaled.x_train[,4]
eliminated$v5 = scaled.x_train[,5]
eliminated$v6 = scaled.x_train[,6]
eliminated$v7 = scaled.x_train[,7]
eliminated$v8 = scaled.x_train[,8]
eliminated$v9 = scaled.x_train[,9]
eliminated$v1 = scaled.x_train[,1]
eliminated$v2 = scaled.x_train[,2]
eliminated$v3 = scaled.x_train[,3]
eliminated$v4 = scaled.x_train[,4]
eliminated$v5 = scaled.x_train[,5]
eliminated$v6 = scaled.x_train[,6]
eliminated$v7 = scaled.x_train[,7]
eliminated$v8 = scaled.x_train[,8]
eliminated$v9 = scaled.x_train[,9]
eliminated$v1 = scaled.x_train[,1]
eliminated$v2 = scaled.x_train[,2]
eliminated$v3 = scaled.x_train[,3]
eliminated$v4 = scaled.x_train[,4]
eliminated$v5 = scaled.x_train[,5]
eliminated$v6 = scaled.x_train[,6]
eliminated$v7 = scaled.x_train[,7]
eliminated$v8 = scaled.x_train[,8]
eliminated$v9 = scaled.x_train[,9]
### REGRESSION ###
# Start with a basic regression using the **lm(y ~ x, data)** function
lm.fit <- lm(scaled.y_train ~ .-v5-v7, data = eliminated)
plot(lm.fit)
### LOADING PACKAGES ###
library(tidyverse) #really dunno
library(eqs2lavaan) # for plotting covariance
library(GGally) # plotting correlation
### LOADING DATA ###
training_data <- as_tibble(read.csv("D:\\Uni\\SL\\SL_Project\\train_ch.csv"))
test_data <- as_tibble(read.csv("D:\\Uni\\SL\\SL_Project\\train_ch.csv"))
x_train <- training_data %>% select(2:10)
y_train <- training_data %>% select(11)
x_test <- test_data %>% select(2:10)
### FEATURE ENGINEERING ###
eliminated <- x_train
eliminated_Y <- y_train
for (v in names(x_train)){
print(v)
# Eliminating outliers
Q <- quantile(x_train[[v]], probs=c(.25, .75), na.rm = FALSE)
iqr <- IQR(x_train[[v]])
up <-  Q[2]+1.5*iqr # Upper Range
low<- Q[1]-1.5*iqr # Lower Range
eliminated_Y <- subset(eliminated_Y, eliminated[[v]] > (Q[1] - 1.5*iqr) & eliminated[[v]] < (Q[2]+1.5*iqr))
eliminated <- subset(eliminated, eliminated[[v]] > (Q[1] - 1.5*iqr) & eliminated[[v]] < (Q[2]+1.5*iqr))
}
# X Standardization
scaled.x_train <- scale(eliminated)
scaled.y_train <- scale(eliminated_Y)
eliminated$v1 = scaled.x_train[,1]
eliminated$v2 = scaled.x_train[,2]
eliminated$v3 = scaled.x_train[,3]
eliminated$v4 = scaled.x_train[,4]
eliminated$v5 = scaled.x_train[,5]
eliminated$v6 = scaled.x_train[,6]
eliminated$v7 = scaled.x_train[,7]
eliminated$v8 = scaled.x_train[,8]
eliminated$v9 = scaled.x_train[,9]
### REGRESSION ###
# Start with a basic regression using the **lm(y ~ x, data)** function
lm.fit <- lm(scaled.y_train ~ .-v5-v7, data = eliminated)
plot(lm.fit)
names(x_train)
names(x_train)[1]
names(x_train)[1][2]
### KNN ###
knn.fit <- knn.reg(train, test = NULL, y, k = 3, algorithm=c("kd_tree", "cover_tree", "brute"))
install.packages("FNN")
names(x_train)[1]
### KNN ###
knn.fit <- knn.reg(train, test = NULL, y, k = 3, algorithm=c("kd_tree", "cover_tree", "brute"))
library(FNN) # knn
### KNN ###
knn.fit <- knn.reg(train, test = NULL, y, k = 3, algorithm=c("kd_tree", "cover_tree", "brute"))
names(x_train)[1]
extract_numeric(names(x_train)[1])
for (v in names(x_train)){
eliminated[[v]] = scaled.x_train[,extract_numeric(v)]
}
install.packages("readr")
library(readr) # string to number
for (v in names(x_train)){
eliminated[[v]] = scaled.x_train[,parse_number(v)]
}
### REGRESSION ###
# Start with a basic regression using the **lm(y ~ x, data)** function
lm.fit <- lm(scaled.y_train ~ .-v5-v7, data = eliminated)
plot(lm.fit)
### KNN ###
knn.fit <- knn.reg(eliminated, test = NULL, y, k = 3, algorithm=c("kd_tree", "cover_tree", "brute"))
### LOADING PACKAGES ###
library(tidyverse) # really dunno
library(eqs2lavaan) # for plotting covariance
library(GGally) # plotting correlation
library(FNN) # knn
library(readr) # string to number
### KNN ###
knn.fit <- knn.reg(eliminated, test = NULL, y, k = 3, algorithm=c("kd_tree", "cover_tree", "brute"))
eleminated$Y = eliminated_Y
eliminated$Y = eliminated_Y
### KNN ###
eliminated_KNN <- eliminated
eliminated_KNN$Y = eliminated_Y
knn.fit <- knn.reg(eliminated_KNN, test = NULL, y, k = 3, algorithm=c("kd_tree", "cover_tree", "brute"))
### KNN ###
# eliminated_KNN <- eliminated
# eliminated_KNN$Y = eliminated_Y
knn.fit <- knn.reg(eliminated_KNN, test = NULL, scaled.y_train, k = 3, algorithm=c("kd_tree", "cover_tree", "brute"))
plot(knn.fit)
### LOADING PACKAGES ###
library(tidyverse) # really dunno
library(eqs2lavaan) # for plotting covariance
library(GGally) # plotting correlation
library(FNN) # knn
library(readr) # string to number
### LOADING DATA ###
training_data <- as_tibble(read.csv("D:\\Uni\\SL\\SL_Project\\train_ch.csv"))
test_data <- as_tibble(read.csv("D:\\Uni\\SL\\SL_Project\\train_ch.csv"))
x_train <- training_data %>% select(2:10)
y_train <- training_data %>% select(11)
x_test <- test_data %>% select(2:10)
y_test <- test_data %>% select(11)
# Eliminating outliers
eliminated <- x_train
eliminated_Y <- y_train
for (v in names(x_train)){
# print(v)
Q <- quantile(x_train[[v]], probs=c(.25, .75), na.rm = FALSE)
iqr <- IQR(x_train[[v]])
up <-  Q[2]+1.5*iqr # Upper Range
low<- Q[1]-1.5*iqr # Lower Range
eliminated_Y <- subset(eliminated_Y, eliminated[[v]] > (Q[1] - 1.5*iqr) & eliminated[[v]] < (Q[2]+1.5*iqr))
eliminated <- subset(eliminated, eliminated[[v]] > (Q[1] - 1.5*iqr) & eliminated[[v]] < (Q[2]+1.5*iqr))
}
# Standardization
scaled.x_train <- scale(eliminated)
scaled.y_train <- scale(eliminated_Y)
for (v in names(x_train)){
eliminated[[v]] = scaled.x_train[,parse_number(v)]
}
### REGRESSION ###
# Start with a basic regression using the **lm(y ~ x, data)** function
lm.fit <- lm(scaled.y_train ~ .-v5-v7, data = eliminated)
### KNN ###
# eliminated_KNN <- eliminated
# eliminated_KNN$Y = eliminated_Y
knn.fit <- knn.reg(eliminated_KNN, test = NULL, scaled.y_train, k = 3, algorithm=c("kd_tree", "cover_tree", "brute"))
### KNN ###
# eliminated_KNN <- eliminated
# eliminated_KNN$Y = eliminated_Y
knn.fit <- knn.reg(eliminated, test = NULL, scaled.y_train, k = 3, algorithm=c("kd_tree", "cover_tree", "brute"))
View(knn.fit)
plot(knn.fit)
plot(scaled.y_train, knn.fit$pred, xlab="y", ylab=expression(hat(scaled.y_train)))
knn.fit <- knn.reg(eliminated, test = NULL, scaled.y_train, k = 3, algorithm=c("kd_tree", "cover_tree", "brute"))
plot(scaled.y_train, knn.fit$pred, xlab="y", ylab=expression(hat(scaled.y_train)))
plot(scaled.y_train)
knn.fit <- knn.reg(eliminated, test = NULL, scaled.y_train, k = 10, algorithm=c("kd_tree", "cover_tree", "brute"))
plot(scaled.y_train, knn.fit$pred, xlab="y", ylab=expression(hat(scaled.y_train)))
knn.fit <- knn.reg(eliminated, test = NULL, scaled.y_train, k = 2, algorithm=c("kd_tree", "cover_tree", "brute"))
plot(scaled.y_train, knn.fit$pred, xlab="y", ylab=expression(hat(scaled.y_train)))
knn.fit <- knn.reg(eliminated, test = NULL, scaled.y_train, k = 2, algorithm=c("kd_tree", "cover_tree", "brute"))
plot(scaled.y_train, knn.fit$pred, xlab="y", ylab=expression(hat(scaled.y_train)))
knn.fit <- knn.reg(eliminated, test = NULL, scaled.y_train, k = 2, algorithm=c("kd_tree", "cover_tree", "brute"))
knn.fit <- knn.reg(eliminated, test = NULL, scaled.y_train, k = 4, algorithm=c("kd_tree", "cover_tree", "brute"))
plot(scaled.y_train, knn.fit$pred, xlab="y", ylab=expression(hat(scaled.y_train)))
# eliminated_KNN <- eliminated
# eliminated_KNN$Y = eliminated_Y
knn.fit <- knn.reg(eliminated, test = NULL, scaled.y_train, k = 4, algorithm=c("kd_tree", "cover_tree", "brute"))
plot(scaled.y_train, knn.fit$pred, xlab="y", ylab=expression(hat(scaled.y_train)))
knn.fit <- knn.reg(eliminated, test = NULL, scaled.y_train, k = 100, algorithm=c("kd_tree", "cover_tree", "brute"))
plot(scaled.y_train, knn.fit$pred, xlab="y", ylab=expression(hat(scaled.y_train)))
### KNN ###
# eliminated_KNN <- eliminated
# eliminated_KNN$Y = eliminated_Y
knn.fit <- knn.reg(eliminated, test = NULL, scaled.y_train, k = 3, algorithm=c("kd_tree", "cover_tree", "brute"))
plot(scaled.y_train, knn.fit$pred, xlab="y", ylab=expression(hat(scaled.y_train)))
knn.fit <- knn.reg(eliminated, test = NULL, scaled.y_train, k = 3, algorithm=c("kd_tree"))
plot(scaled.y_train, knn.fit$pred, xlab="y", ylab=expression(hat(scaled.y_train)))
knn.fit <- knn.reg(eliminated, test = NULL, scaled.y_train, k = 3, algorithm=c( "cover_tree"))
plot(scaled.y_train, knn.fit$pred, xlab="y", ylab=expression(hat(scaled.y_train)))
knn.fit <- knn.reg(eliminated, test = NULL, scaled.y_train, k = 3, algorithm=c("brute"))
plot(scaled.y_train, knn.fit$pred, xlab="y", ylab=expression(hat(scaled.y_train)))
knn.fit <- knn.reg(eliminated, test = NULL, scaled.y_train, k = 300, algorithm=c("kd_tree", "cover_tree", "brute"))
plot(scaled.y_train, knn.fit$pred, xlab="y", ylab=expression(hat(scaled.y_train)))
knn.fit <- knn.reg(eliminated, test = NULL, scaled.y_train, k = 900, algorithm=c("kd_tree", "cover_tree", "brute"))
plot(scaled.y_train, knn.fit$pred, xlab="y", ylab=expression(hat(scaled.y_train)))
knn.fit <- knn.reg(eliminated, test = NULL, scaled.y_train, k = 800, algorithm=c("kd_tree", "cover_tree", "brute"))
plot(scaled.y_train, knn.fit$pred, xlab="y", ylab=expression(hat(scaled.y_train)))
View(scaled.y_train)
pred_bread = knn.fit$pred
pred_bread
plot(pred_bread)
plot(scaled.y_train)
plot(pred_bread)
plot(scaled.y_train)
mean(scaled.y_train)
mean(pred_bread)
# eliminated_KNN$Y = eliminated_Y
knn.fit <- knn.reg(eliminated, test = NULL, scaled.y_train, k = 10, algorithm=c("kd_tree", "cover_tree", "brute"))
plot(scaled.y_train, knn.fit$pred, xlab="y", ylab=expression(hat(scaled.y_train)))
### LOADING PACKAGES ###
library(tidyverse) # really dunno
library(eqs2lavaan) # for plotting covariance
library(GGally) # plotting correlation
library(FNN) # knn
library(readr) # string to number
### LOADING DATA ###
training_data <- as_tibble(read.csv("D:\\Uni\\SL\\SL_Project\\train_ch.csv"))
test_data <- as_tibble(read.csv("D:\\Uni\\SL\\SL_Project\\train_ch.csv"))
x_train <- training_data %>% select(2:10)
y_train <- training_data %>% select(11)
x_test <- test_data %>% select(2:10)
y_test <- test_data %>% select(11)
# Eliminating outliers
eliminated <- x_train
eliminated_Y <- y_train
eliminated <- x_train
eliminated_Y <- y_train
for (v in names(x_train)){
# print(v)
Q <- quantile(x_train[[v]], probs=c(.25, .75), na.rm = FALSE)
iqr <- IQR(x_train[[v]])
up <-  Q[2]+1.5*iqr # Upper Range
low<- Q[1]-1.5*iqr # Lower Range
eliminated_Y <- subset(eliminated_Y, eliminated[[v]] > (Q[1] - 1.5*iqr) & eliminated[[v]] < (Q[2]+1.5*iqr))
eliminated <- subset(eliminated, eliminated[[v]] > (Q[1] - 1.5*iqr) & eliminated[[v]] < (Q[2]+1.5*iqr))
}
# Standardization
scaled.x_train <- scale(eliminated)
scaled.y_train <- scale(eliminated_Y)
for (v in names(x_train)){
eliminated[[v]] = scaled.x_train[,parse_number(v)]
}
### REGRESSION ###
# Start with a basic regression using the **lm(y ~ x, data)** function
lm.fit <- lm(scaled.y_train ~ .-v5-v7, data = eliminated)
plot(lm.fit)
# eliminated_KNN$Y = eliminated_Y
knn.fit <- knn.reg(eliminated, test = NULL, scaled.y_train, k = 10, algorithm=c("kd_tree", "cover_tree", "brute"))
plot(scaled.y_train, knn.fit$pred, xlab="y", ylab=expression(hat(scaled.y_train)))
library(tensorflow)
install_tensorflow(version = 'gpu')
install.packages("tensorflow")
install_tensorflow(version = 'gpu')
library(tensorflow)
install_tensorflow(version = 'gpu')
y
library(tensorflow)
hello <- tf$constant("Hello")
print(hello)
library(purrr)
library(keras)
library(tfdatasets)
library(tfautograph)
library(reticulate)
library(purrr)
install.packages("keras")
library(keras)
library(tfdatasets)
library(tfautograph)
library(reticulate)
library(purrr)
mnist <- dataset_mnist()
mnist$train$x <- mnist$train$x/255
mnist$test$x <- mnist$test$x/255
dim(mnist$train$x) <- c(dim(mnist$train$x), 1)
dim(mnist$test$x) <- c(dim(mnist$test$x), 1)
train_ds <- mnist$train %>%
tensor_slices_dataset() %>%
dataset_take(20000) %>%
dataset_map(~modify_at(.x, "x", tf$cast, dtype = tf$float32)) %>%
dataset_map(~modify_at(.x, "y", tf$cast, dtype = tf$int64)) %>%
dataset_shuffle(10000) %>%
dataset_batch(32)
test_ds <- mnist$test %>%
tensor_slices_dataset() %>%
dataset_take(2000) %>%
dataset_map(~modify_at(.x, "x", tf$cast, dtype = tf$float32)) %>%
dataset_map(~modify_at(.x, "y", tf$cast, dtype = tf$int64)) %>%
dataset_batch(32)
train_ds <- mnist$train %>%
tensor_slices_dataset() %>%
dataset_take(20000) %>%
dataset_map(~modify_at(.x, "x", tf$cast, dtype = tf$float32)) %>%
dataset_map(~modify_at(.x, "y", tf$cast, dtype = tf$int64)) %>%
dataset_shuffle(10000) %>%
dataset_batch(32)
library(keras)
library(tfdatasets)
library(tfautograph)
library(reticulate)
library(purrr)
mnist <- dataset_mnist()
mnist$train$x <- mnist$train$x/255
mnist$test$x <- mnist$test$x/255
dim(mnist$train$x) <- c(dim(mnist$train$x), 1)
dim(mnist$test$x) <- c(dim(mnist$test$x), 1)
train_ds <- mnist$train %>%
tensor_slices_dataset() %>%
dataset_take(20000) %>%
dataset_map(~modify_at(.x, "x", tf$cast, dtype = tf$float32)) %>%
dataset_map(~modify_at(.x, "y", tf$cast, dtype = tf$int64)) %>%
dataset_shuffle(10000) %>%
dataset_batch(32)
test_ds <- mnist$test %>%
tensor_slices_dataset() %>%
dataset_take(2000) %>%
dataset_map(~modify_at(.x, "x", tf$cast, dtype = tf$float32)) %>%
dataset_map(~modify_at(.x, "y", tf$cast, dtype = tf$int64)) %>%
dataset_batch(32)
simple_conv_nn <- function(filters, kernel_size) {
keras_model_custom(name = "MyModel", function(self) {
self$conv1 <- layer_conv_2d(
filters = filters,
kernel_size = rep(kernel_size, 2),
activation = "relu"
)
self$flatten <- layer_flatten()
self$d1 <- layer_dense(units = 128, activation = "relu")
self$d2 <- layer_dense(units = 10, activation = "softmax")
function(inputs, mask = NULL) {
inputs %>%
self$conv1() %>%
self$flatten() %>%
self$d1() %>%
self$d2()
}
})
}
model <- simple_conv_nn(filters = 32, kernel_size = 3)
train_ds <- mnist$train %>%
tensor_slices_dataset() %>%
dataset_take(20000) %>%
dataset_map(~modify_at(.x, "x", tf$cast, dtype = tf$float32)) %>%
dataset_map(~modify_at(.x, "y", tf$cast, dtype = tf$int64)) %>%
dataset_shuffle(10000) %>%
dataset_batch(32)
loss <- loss_sparse_categorical_crossentropy
optimizer <- optimizer_adam()
train_loss <- tf$keras$metrics$Mean(name='train_loss')
train_accuracy <-  tf$keras$metrics$SparseCategoricalAccuracy(name='train_accuracy')
test_loss <- tf$keras$metrics$Mean(name='test_loss')
test_accuracy <- tf$keras$metrics$SparseCategoricalAccuracy(name='test_accuracy')
train_step <- function(images, labels) {
with (tf$GradientTape() %as% tape, {
predictions <- model(images)
l <- loss(labels, predictions)
})
gradients <- tape$gradient(l, model$trainable_variables)
optimizer$apply_gradients(purrr::transpose(list(
gradients, model$trainable_variables
)))
train_loss(l)
train_accuracy(labels, predictions)
}
test_step <- function(images, labels) {
predictions <- model(images)
l <- loss(labels, predictions)
test_loss(l)
test_accuracy(labels, predictions)
}
training_loop <- tf_function(autograph(function(train_ds, test_ds) {
for (b1 in train_ds) {
train_step(b1$x, b1$y)
}
for (b2 in test_ds) {
test_step(b2$x, b2$y)
}
tf$print("Acc", train_accuracy$result(), "Test Acc", test_accuracy$result())
train_loss$reset_states()
train_accuracy$reset_states()
test_loss$reset_states()
test_accuracy$reset_states()
}))
for (epoch in 1:5) {
cat("Epoch: ", epoch, " -----------\n")
training_loop(train_ds, test_ds)
}
### LOADING PACKAGES ###
library(tidyverse) # really dunno
library(eqs2lavaan) # for plotting covariance
library(GGally) # plotting correlation
library(FNN) # knn
library(readr) # string to number
library(Metrics)
library(caret) # normalization
library(Directional) # another knn (regression tuning)
### LOADING DATA ###
setwd("D:\\Uni\\SL\\SL_Project_2\\")
ADCN_train <- as_tibble(read.csv(".\\dataset\\ADCNtrain.csv"))
ADCN_test <- as_tibble(read.csv(".\\dataset\\ADCNtest.csv"))
ADMCI_train <- as_tibble(read.csv(".\\dataset\\ADMCItrain.csv"))
ADMCI_test <- as_tibble(read.csv(".\\dataset\\ADMCItest.csv"))
MCICN_train <- as_tibble(read.csv(".\\dataset\\MCICNtrain.csv"))
MCICN_test <- as_tibble(read.csv(".\\dataset\\MCICNtest.csv"))
View(ADCN_train)
ADCN_test.names
ADCN_test.colnames
colnames(ADCN_test)
View(ADMCI_train)
ADMCI$Background
ADMCI_train$Background
